{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.113878Z",
     "start_time": "2025-02-11T03:16:01.110374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ],
   "id": "4cc71d9d7fc1290b",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.135689Z",
     "start_time": "2025-02-11T03:16:01.133680Z"
    }
   },
   "cell_type": "code",
   "source": "from emotion_model import LandmarkEmotionModel",
   "id": "81d6acefa1d815fd",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.140598Z",
     "start_time": "2025-02-11T03:16:01.138694Z"
    }
   },
   "cell_type": "code",
   "source": "from landmarks import FaceLandmarks",
   "id": "880af36333eae811",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.147855Z",
     "start_time": "2025-02-11T03:16:01.144960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "55e9c7d5f19d8656",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.156632Z",
     "start_time": "2025-02-11T03:16:01.153889Z"
    }
   },
   "source": [
    "path = './models/'\n",
    "name = 'landmark_emotion_model.pt'\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.180594Z",
     "start_time": "2025-02-11T03:16:01.163065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LandmarkEmotionModel().to(device)\n",
    "model.load_state_dict(torch.load(path + name))\n",
    "model.eval()"
   ],
   "id": "31b4c261869f51d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LandmarkEmotionModel(\n",
       "  (layer_1): Linear(in_features=956, out_features=64, bias=True)\n",
       "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.190285Z",
     "start_time": "2025-02-11T03:16:01.186574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_emotion(image):\n",
    "\n",
    "    landmarks = FaceLandmarks().detect_face_landmarks(image)\n",
    "\n",
    "    if landmarks is None or len(landmarks) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    data = np.array(landmarks).flatten()\n",
    "    landmarks_tensor = torch.tensor(data, dtype=torch.float32).view(1, -1).to(device)\n",
    "    # Assuming the model prediction already exists\n",
    "    with torch.no_grad():\n",
    "        prediction = model(landmarks_tensor)\n",
    "\n",
    "        # If using a binary classification task (sigmoid output), apply the sigmoid function\n",
    "        probability = torch.sigmoid(prediction).item()\n",
    "\n",
    "        # For classification, the threshold is usually 0.5 for binary classification\n",
    "        predicted_class = 1 if probability > 0.5 else 0\n",
    "\n",
    "        # percent = probability * 100\n",
    "\n",
    "        return predicted_class, probability, landmarks"
   ],
   "id": "7bff4234890fd787",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluate the model Live\n",
    "\n",
    "- note 1 is angry and 0 is not angry"
   ],
   "id": "c01c42719f127fb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.198281Z",
     "start_time": "2025-02-11T03:16:01.195591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ],
   "id": "bc7a6684ab398fbe",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.738027Z",
     "start_time": "2025-02-11T03:16:01.204086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to webcam\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)"
   ],
   "id": "f963686e0a49dbd4",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:16:01.746233Z",
     "start_time": "2025-02-11T03:16:01.743720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "else:\n",
    "    print(\"Webcam opened successfully!\")"
   ],
   "id": "eedf98503a782a91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam opened successfully!\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:17:48.351958Z",
     "start_time": "2025-02-11T03:16:01.814418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loop through every frame until we close our webcam\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    pred, prob, landmarks = predict_emotion(frame)\n",
    "\n",
    "    if landmarks is None or len(landmarks) == 0:\n",
    "        text =f\"Face not detected\"\n",
    "        # Display text on the frame\n",
    "        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('Webcam', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        continue\n",
    "\n",
    "    # Display text\n",
    "    pred_emotion = \"\"\n",
    "\n",
    "    if pred == 1:\n",
    "        pred_emotion = \"Angry\"\n",
    "    else:\n",
    "        pred_emotion = \"Not Angry\"\n",
    "\n",
    "    text =f\"Angry: {pred_emotion} ({prob*100:.2f}%)\"\n",
    "\n",
    "    # Display text on the frame\n",
    "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display landmarks\n",
    "    for x,y in landmarks:\n",
    "        cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "    # Show image\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    # Checks whether q has been hit and stops the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Releases the webcam\n",
    "cap.release()\n",
    "# Closes the frame\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "f898c957ef8fbc2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n",
      "No face detected!\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
